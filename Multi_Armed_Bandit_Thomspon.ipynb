{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-Armed Bandit Thomspon.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJ/bISj2YsI8D9pfpkVCai",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FelixFelicis555/Multi-Arm-Bandit-Algorithm/blob/main/Multi_Armed_Bandit_Thomspon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi-Armed Bandit Thompson-Sampling Algorithm**:\n",
        "        \n",
        "\n",
        "*   To solve Exploit-Explore Problem in picking advertising.\n",
        "*   Results in 5-6% revenue growth in advertizing by getting high effective high effective CTR(Click Through Rate)\n",
        "\n",
        "\n",
        "\n",
        "*   The main-objective is ,given a bunch of bandits that gives different rewards,to identify the one that gives highest-ones as fast as possible\n",
        "*   Thomspson-Sampling,otherwise known as Bayesian Bandits\n",
        "* It follows two distributions-Beta & Binomial for Prior and Likelihood(As it's in conjugate prior)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tnqW_c4VMJNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It follows Bayesian Approach to Multi-Arm Bandit\n",
        "Treat 'Î¼' , average reward from each bandit as random variable\n",
        "To calculate Distribution,we use data  we have collected so far\n",
        "* Sample the Data-points from each bandits average reward distribution\n",
        "* Select the one whose sample had highest value\n",
        "* Subsequently update the average reward distribution of selected bandit\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VlMEpdHyRYFD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuklCKgFI81e"
      },
      "outputs": [],
      "source": [
        "# Defining Base Class for Bernoulli-Bandit that represents bandit,Agent to represent agent \n",
        "# BanditRewardsLog-To keep Track ofthe rewards\n",
        "import logging\n",
        "from abc import\n",
        "{\n",
        "    ABC,\n",
        " abstractmethod,\n",
        "}\n",
        "from collections import defaultdict\n",
        "from typing import List\n",
        "from uuid import uuid4\n",
        "import numpy as np\n",
        "from samples.r1.errors import NoBanditsError\n",
        "logger=logging.getLogger(__name__)\n",
        "class BernoulliBandit:\n",
        "  def __init__(self,p):\n",
        "    '''\n",
        "     Simulates Bandit.\n",
        "     Args:\n",
        "         p: probability of success\n",
        "    '''\n",
        "    self.p=p\n",
        "    self.id=uuid4()\n",
        "  def pull(self):\n",
        "    \"\"\"\n",
        "     Simulate pulling the arm of bandit\n",
        "    \"\"\" \n",
        "    return np.random.binomial(1,self.p,size=1)[0] \n",
        "\n",
        "class BanditRewardsLog:\n",
        "  def __int__(self):\n",
        "    self.total_actions=0\n",
        "    self.total_rewards=0\n",
        "    self.all_rewards=[]\n",
        "    self.record=defaultdict(lambda:dict(actions=0,rewards=0,reward_squared=0))\n",
        "  def __getitem__(self,bandit):\n",
        "    return self.record[bandit.id]\n",
        "  def record_action(self,bandit,reward):\n",
        "    self.total_actions+=1\n",
        "    self.total_rewards+=1\n",
        "    self.all_rewards.append(reward)\n",
        "    self.record[bandit.id]['actions']+=1\n",
        "    self.record[bandit.id]['reward']+=reward\n",
        "    self.record[bandit.id]['reward_squared']+=reward**2\n",
        "class Agent(ABC):\n",
        "  def __init__(self):\n",
        "    self.rewards_log=BanditRewardsLog()\n",
        "    self._bandits=None\n",
        "    def bandits(self):\n",
        "      if not self._bandits:\n",
        "        raise NoBanditsError()\n",
        "      return self._bandits\n",
        "    def bandits(self,val):\n",
        "      self._bandits=val\n",
        "    def take_action(self):\n",
        "\n",
        "    def take_actions(self,n):\n",
        "      for _ in range(n):\n",
        "        self.take_action()        \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Thomspon-Sampling Implementation by taking sub-class as agent\n",
        "from scipy import stats\n",
        "class BayesianAgent(Agent):\n",
        "  def __init__(self,reward_distr='bernoulli'):\n",
        "    if reward_distr not in ('bernoulli'):\n",
        "      raise ValueError('reward_distr must be \"bernoulli\".')\n",
        "    self.reward_distr=reward_distr\n",
        "    super().__init__()\n",
        "  def _sample_bandit_mean(self,bandit):\n",
        "    bandit_record=self.rewards_log[bandit]\n",
        "    if self.reward_distr=='bernoulli':\n",
        "      success=bandit_record['reward']+1\n",
        "      failures=bandit_record['actions']-bandit_record['rewards']+1\n",
        "      return np.random.beta(a=success,b=failures,size=1)[0]\n",
        "    else:\n",
        "       raise NotImplementedError() \n",
        "   def take_actions(self):\n",
        "     samples=[self._sample_bandit_mean(bandit) for bandit in self.bandits]\n",
        "     current_bandit=self.bandit[np.argmax(samples)] \n",
        "     reward=current_bandit.pull()\n",
        "     self.rewards_log.record_action(current_bandit,reward)\n",
        "     return reward\n",
        "    def __repr__(self):\n",
        "      return 'BayesianAgent(reward_distr=\"{}\")'.format(self.reward_distr)          "
      ],
      "metadata": {
        "id": "6_uc6J6NaclC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *There are different bandit algorithms out there like Epsilon-Greedy,Optimistic Initial Values,UCB,UGB1-Tuned*\n",
        "\n",
        "* You and Your friend's have been using bandit lagorithms to optmize which restaurants and movies to choose for your weekly movie night,Let's Experiment with all these bandit algorithms\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z0KgzgPVPAq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "from typing import List\n",
        "import matplotlib.pyplot as plt\n",
        "from samples.r1 import ucb\n",
        "from samples.r1.bandit import(\n",
        "    Agent,\n",
        "    Bandit,\n",
        "    BernoulliBandit\n",
        ")\n",
        "from samples.r1.epsilon_greedy import EpsilonGreedyAgent\n",
        "from samples.r1.optimistic_initial_values import OptimisticInitialValuesAgent\n",
        "from samples.r1.thompson_sampling import BayesianAgent\n",
        "logger=logging.getLogger(__name__)\n",
        "def compare_agents(agents,bandits,iterations,show_plot=True):\n",
        "  for agent in agents:\n",
        "    logger.info(\"Running for agent=%s\",agent)\n",
        "    agent.bandits=bandits\n",
        "    if isInstance(agent,ucb.UCBAgent):\n",
        "      agent.initialize()\n",
        "    N=iterations-agent-rewards_log.total_actions\n",
        "    agent.take_actions(N)\n",
        "        if show_plot:\n",
        "            cumulative_rewards = np.cumsum(\n",
        "                agent.rewards_log.all_rewards,\n",
        "            )\n",
        "            plt.plot(cumulative_rewards, label=str(agent))\n",
        "\n",
        "    if show_plot:\n",
        "        plt.xlabel(\"iteration\")\n",
        "        plt.ylabel(\"total rewards\")\n",
        "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "        plt.show()\n",
        "\n",
        "  \n",
        " def get_agents():\n",
        "    agents = [\n",
        "        EpsilonGreedyAgent(),\n",
        "        ucb.UCB1Agent(),\n",
        "        ucb.UCB1TunedAgent(),\n",
        "        BayesianAgent('bernoulli'),\n",
        "    ]\n",
        "    return agents\n",
        "\n",
        "\n",
        "def run_comparison(bandits):\n",
        "    win_count = [0] * len(get_agents())\n",
        "    \n",
        "    for _ in range(1000):\n",
        "        agents = get_agents()\n",
        "        iterations = 1000\n",
        "        compare_agents(agents, bandits, iterations, show_plot=False)\n",
        "    \n",
        "        rewards = [agent.rewards_log.total_rewards for agent in agents]\n",
        "        win_count[np.argmax(rewards)] += 1\n",
        "        \n",
        "    return win_count  "
      ],
      "metadata": {
        "id": "CMmItaT1dEGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = [0.6, 0.7, 0.8, 0.9]\n",
        "bernoulli_bandits = [BernoulliBandit(p) for p in probs]"
      ],
      "metadata": {
        "id": "2ay06zvWfVBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_agents(\n",
        "  get_agents(), \n",
        "  bernoulli_bandits, \n",
        "  iterations=500, \n",
        "  show_plot=True,\n",
        ")"
      ],
      "metadata": {
        "id": "YfdSuXOffaVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}